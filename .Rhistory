par0 <- par()
par0()
par0
par(par0)
par0 <- par()
par(par0)
par(par=par0)
plot(c(1,1))
par() <- par0
par0
par(par0)
library(devtools)
library(roxygen2)
warnings()
Sys.setenv('http_proxy'='http://proxyout.lanl.gov:8080')
Sys.setenv('https_proxy'='http://proxyout.lanl.gov:8080')
Ibeta
?Ibeta
zipfR::Ibeta
beta
curve(dbeta(x, 1, 1.54))
curve(dbeta(x, 1, 1.54))
curve(dbeta(x, 11.5, 1))
curve(dbeta(x, 1.5, 1))
curve(dbeta(x, 1, 0.5))
curve(dbeta(x, 1.1, 1))
importFrom("stats")
document()
?runif
?plot
document()
document()
document()
document()
document()
document()
document()
document()
document()
?C_bass
Cfg_bass
?Cfg_bass
?lm
library(stats)
document()
?`:::`
document()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
f <- function(x)
x[1]^2 + x[1]*x[2] + x[2]^3/9
n <- 500
x <- matrix(runif(3*n), nrow=n)
y <- apply(x, 1, f)
library(BASS)
mod <- bass(x, y)
C <- C_bass(mod)
C <- C_bass(mod)
C <- C_bass(mod)
C
120/45
C <- C_bass(mod)
C/45
C <- C_bass(mod)
C*45
C <- C_bass(mod)
round(C*45, 3)
document()
?concordance::C_bass
plot_active_grad_k
eigen(C)
plot_active_grad_k(C)
?plot_active_grad_k
class(C)
act_scores()
?act_scores()
act_scores(C)
act_scores(C)
act_scores(C, k=3)
?plot_sensitivities()
document()
document()
install.packages("badger")
vignette()
vignette(package="concordance")
browseVignettes("concordance")
library(concordance)
browseVignettes("concordance")
library(concordance)
browseVignettes('concordance')
build_vignettes()
browseVignettes("concordance")
tools::buildVignettes(dir = ".", tangle=TRUE)
dir.create("inst/doc")
file.copy(dir("vignettes", full.names=TRUE), "inst/doc", overwrite=TRUE)
browseVignettes()
browseVignettes("concordance")
Version()
version()
version
y <- apply(X, 1, duqling::dms_additive)
X <- matrix(runif(2000), ncol=2)
y <- apply(X, 1, duqling::dms_additive)
dim(X)
mod1 <- bass(X[1:500,], y[1:500])
library(BASS)
mod1 <- bass(X[1:500,], y[1:500])
mod2 <- bass(X, y)
plot(mod1)
plot(mod2)
library(concordance)
C1 <- C_bass(mod1)
C2 <- C_bass(mod2)
C12 <- C_fg(mod1, mod2)
C12 <- Cfg_bass(mod1, mod2)
C12 <- Cfg_bass(mod2, mod1)
tr <- function(xx) sum(diag(xx))
tr(C12)/sqrt(tr(C1)*tr(C2))
tr <- function(xx) sum(diag(xx))
conc_C <- function(C1, C2, C12){
tr(C12)/sqrt(tr(C1)*tr(C2))
}
library(parallel)
detectCores
detectCores()
tr <- function(xx) sum(diag(xx))
conc_C <- function(C1, C2, C12){
tr(C12)/sqrt(tr(C1)*tr(C2))
}
# Full model should come first
concordance <- function(mod1, mod2){
C1 <- C_bass(mod1)
C2 <- C_bass(mod2)
C12 <- Cfg_bass(mod1, mod1)
conc_C(C1, C2, C12)
}
library(BASS)
library(concordance)
library(parallel)
library(lhs)
X <- matrix(lhs::maximinLHS(1000, 2), ncol=2)
y <- apply(X, 1, duqling::dms_additive)
mod<-bass(X,y) # full model
nfolds<-20 # Iâ€™ll break the training data into 20 parts
scramble<-sample(nfolds,size=length(y),replace=T) # randomly allocate training data into folds
fit<-function(i){
ind<-which(scramble<=i) # if i=5, train with the first 5 folds
bass(X[ind,],y[ind])
}
mods<-parallel::mclapply(1:nfolds,fit,mc.cores = 5)
#qoi<-unlist(lapply(mods, function(m) mean(sqrt(m$s2/mod$s2)))) # for each model, get the relative error (relative to the full model)
# would rather have concordance as the qoi:
qoi <- unlist(lapply(mods, function(m) concordance(mod, m))))
#qoi<-unlist(lapply(mods, function(m) mean(sqrt(m$s2/mod$s2)))) # for each model, get the relative error (relative to the full model)
# would rather have concordance as the qoi:
qoi <- unlist(lapply(mods, function(m) concordance(mod, m)))
qoi
plot(cumsum(table(scramble)),qoi,xlab='sample size')
# Full model should come first
concordance <- function(mod1, mod2){
C1 <- C_bass(mod1)
C2 <- C_bass(mod2)
C12 <- Cfg_bass(mod1, mod2)
conc_C(C1, C2, C12)
}
#qoi<-unlist(lapply(mods, function(m) mean(sqrt(m$s2/mod$s2)))) # for each model, get the relative error (relative to the full model)
# would rather have concordance as the qoi:
qoi <- unlist(lapply(mods, function(m) concordance(mod, m)))
plot(cumsum(table(scramble)),qoi,xlab='sample size')
cor(qoi, 1:20)
?Cfg_bass
mod
mods
length(mods)
mods[[1]]
concordance(mod, mods[[1]])
concordance(mod, mods[[2]])
concordance(mod, mods[[3]])
concordance(mod, mods[[4]])
library(BASS)
library(concordance)
library(parallel)
library(lhs)
tr <- function(xx) sum(diag(xx))
conc_C <- function(C1, C2, C12){
tr(C12)/sqrt(tr(C1)*tr(C2))
}
# Full model should come first
concordance <- function(mod1, mod2){
C1 <- C_bass(mod1)
C2 <- C_bass(mod2)
C12 <- Cfg_bass(mod1, mod2)
conc_C(C1, C2, C12)
}
X <- matrix(lhs::maximinLHS(1000, 2), ncol=2)
y <- apply(X, 1, duqling::dms_additive)
mod<-bass(X,y) # full model
nfolds<-20 # Iâ€™ll break the training data into 20 parts
scramble<-sample(nfolds,size=length(y),replace=T) # randomly allocate training data into folds
fit<-function(i){
ind<-which(scramble<=i) # if i=5, train with the first 5 folds
bass(X[ind,],y[ind])
}
mods<-parallel::mclapply(1:nfolds,fit,mc.cores = 5)
#qoi<-unlist(lapply(mods, function(m) mean(sqrt(m$s2/mod$s2)))) # for each model, get the relative error (relative to the full model)
# would rather have concordance as the qoi:
qoi <- unlist(lapply(mods, function(m) concordance(mod, m)))
plot(cumsum(table(scramble)),qoi,xlab='sample size')
concordance(mod, mods[[1]])
concordance(mod, mods[[2]])
concordance(mod, mods[[3]])
?bass
bass(X, y, curr.list=mods[[1]])
bass(X, y, curr.list=mods[[1]]$curr.list, nmcmc=1)
bass(X, y, curr.list=mods[[1]]$curr.list, nmcmc=1, nburn=1)
bass(X, y, curr.list=mods[[1]]$curr.list, nmcmc=1, nburn=0)
mods[[1]]$curr.list
names(mods[[1]]$curr.list)
class(mods[[1]]$curr.list)
length(mods[[1]]$curr.list)
mods[[1]]$curr.list[[1]]
length(mods[[1]]$curr.list[[1]])
concordance(mod, mods[[1]])
concordance(mod, mods[[5]])
concordance(mod, mods[[6]])
concordance(mods[[6]], mod)
concordance(mod, mods[[5]])
bass(X, y, curr.list=mods[[1]]$curr.list[[1]], nmcmc=1, nburn=1)
bass(X, y, curr.list=mods[[1]]$curr.list[[1]], nmcmc=1, nburn=0)
bass(X, y, curr.list=mods[[1]]$curr.list
, nmcmc=1, nburn=0)
bass(X, y, curr.list=mods[[1]]$curr.list, nmcmc=2, nburn=1)
library(BASS)
library(concordance)
library(parallel)
library(lhs)
tr <- function(xx) sum(diag(xx))
conc_C <- function(C1, C2, C12){
tr(C12)/sqrt(tr(C1)*tr(C2))
}
# Full model should come first
concordance <- function(mod1, mod2){
C1 <- C_bass(mod1)
C2 <- C_bass(mod2)
C12 <- Cfg_bass(mod1, mod2)
conc_C(C1, C2, C12)
}
X <- matrix(lhs::maximinLHS(10000, 2), ncol=2)
X <- matrix(lhs::randomLHS(10000, 2), ncol=2)
y <- apply(X, 1, duqling::dms_additive)
mod<-bass(X,y) # full model
nfolds<-20 # Iâ€™ll break the training data into 20 parts
scramble<-sample(nfolds,size=length(y),replace=T) # randomly allocate training data into folds
fit<-function(i){
ind<-which(scramble<=i) # if i=5, train with the first 5 folds
bass(X[ind,],y[ind])
}
mods<-parallel::mclapply(1:nfolds,fit,mc.cores = 5)
#qoi<-unlist(lapply(mods, function(m) mean(sqrt(m$s2/mod$s2)))) # for each model, get the relative error (relative to the full model)
# would rather have concordance as the qoi:
qoi <- unlist(lapply(mods, function(m) concordance(mod, m)))
plot(cumsum(table(scramble)),qoi,xlab='sample size')
mod1 <- mod
mod2 <- mods[[5]]
C1 <- C_bass(mod1)
C2 <- C_bass(mod2)
C12 <- Cfg_bass(mod1, mod2)
C1
C2
C12
eigen(C1)
eigen(C2)
eigen(C12)
library(rjson)
library(concordance)
library(BASS)
library(stargazer)
tr <- function(M) sum(diag(M))
fnames <- c("gold.csv", "ss304.csv", "al6061.csv",  "uranium.csv", "nickel.csv", "ss4340.csv", "al7075.csv", "ss250.csv",  "copper.csv")
data <- fromJSON(file = "data/pdv.json")
library(rjson)
library(concordance)
library(BASS)
data <- fromJSON(file = "data/pdv.json")
